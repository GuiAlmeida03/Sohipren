# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PKSOgJGxh49W1v_VnqE67blPHnH_zCqq
"""



# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import traceback # Para detalhes de erros
import io # To handle BytesIO from uploaded file

# Imports específicos para séries temporais e diagnóstico
from statsmodels.tsa.seasonal import seasonal_decompose
import statsmodels.api as sm # Para qqplot
from statsmodels.graphics.tsaplots import plot_acf # Para ACF dos resíduos
from pmdarima import auto_arima # Importação chave para otimização SARIMA
from prophet import Prophet
from prophet.plot import plot_plotly, plot_components_plotly # Optional nice plots

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
from scipy import stats # Para usar stats.norm

# NEW: Import for simpler product forecast if needed
from statsmodels.tsa.holtwinters import ExponentialSmoothing

warnings.filterwarnings('ignore')

# Configurações de Visualização Melhoradas (will be overridden by Streamlit sometimes)
plt.style.use('seaborn-v0_8-darkgrid')
plt.rcParams['figure.figsize'] = (12, 6) # Slightly smaller default for Streamlit
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['axes.labelsize'] = 10
plt.rcParams['xtick.labelsize'] = 9
plt.rcParams['ytick.labelsize'] = 9
plt.rcParams['legend.fontsize'] = 9
sns.set_palette('viridis')


class FaturamentoForecast:
    """
    Classe para análise e previsão de séries temporais de faturamento,
    com suporte para SARIMA (via auto_arima) e Prophet.
    Suporta variáveis exógenas (regressors).
    MODIFIED FOR STREAMLIT: Plotting methods return figures. Added individual product forecast.
    """
    def __init__(self, file_input, coluna_data='EMISSÃO', coluna_valor='VALOR TOTAL'):
        """
        Inicializa a classe FaturamentoForecast.

        Args:
            file_input (str, Path, BytesIO or pd.DataFrame): Caminho para o arquivo Excel,
                                                                objeto BytesIO do arquivo carregado,
                                                                ou DataFrame já carregado.
            coluna_data (str): Nome da coluna contendo as datas.
            coluna_valor (str): Nome da coluna contendo os valores a serem previstos.
        """
        self.file_input = file_input # Can be path or BytesIO or DataFrame
        self.coluna_data = coluna_data
        self.coluna_valor = coluna_valor
        self.df_raw = None
        self.df_agregado = None
        self.df_exog = None # Para armazenar exógenas agregadas
        self.exog_cols = [] # Nomes das colunas exógenas
        self.modelo_auto_arima = None
        self.modelo_prophet = None
        self.forecast_prophet = None
        self.metricas = {}
        self.metricas_avancadas = {}
        self.previsoes_futuras_df = None
        self._last_agg_freq = None
        self._rename_col_agg = None
        self.top_produtos_list = [] # NEW: Store top product names
        self.top_clientes_list = [] # NEW: Store top client names


    def carregar_dados(self, df_exogenas=None):
        """
        Carrega dados do input (arquivo, BytesIO ou DataFrame) e opcionalmente variáveis exógenas.
        Realiza limpeza básica (datas, nulos no valor).
        Armazena os nomes das colunas exógenas em self.exog_cols.

        Args:
            df_exogenas (pd.DataFrame, opcional): DataFrame contendo variáveis
                exógenas. DEVE TER um índice Datetime alinhado com a coluna_data
                após a remoção de NaTs nos dados principais.

        Returns:
            pd.DataFrame or None: DataFrame bruto carregado e tratado ou None em caso de erro.
        """
        print(f"\n--- 1. Carregando dados ---") # Simplified message
        try:
            # Handle different input types
            if isinstance(self.file_input, pd.DataFrame):
                 print("   Lendo dados de DataFrame fornecido.")
                 self.df_raw = self.file_input.copy()
            elif isinstance(self.file_input, io.BytesIO):
                 print("   Lendo dados de BytesIO (arquivo carregado).")
                 self.df_raw = pd.read_excel(self.file_input)
            elif isinstance(self.file_input, str):
                 print(f"   Lendo dados do arquivo: '{self.file_input}'")
                 self.df_raw = pd.read_excel(self.file_input)
            else:
                 raise ValueError("Input de dados inválido. Forneça caminho, BytesIO ou DataFrame.")

            print(f"Arquivo carregado. Formato inicial: {self.df_raw.shape}")

            # Verificação de colunas (Case-insensitive check)
            self.df_raw.columns = self.df_raw.columns.str.strip() # Remove leading/trailing spaces

            # Find columns case-insensitively if exact match fails
            def find_col(target_col, df_cols):
                if target_col in df_cols:
                    return target_col
                for col in df_cols:
                    if col.lower() == target_col.lower():
                        print(f"   Aviso: Coluna '{target_col}' encontrada como '{col}' (case-insensitive).")
                        return col
                return None

            found_data_col = find_col(self.coluna_data, self.df_raw.columns)
            found_valor_col = find_col(self.coluna_valor, self.df_raw.columns)

            if not found_data_col:
                 raise ValueError(f"Coluna de data '{self.coluna_data}' (ou similar) não encontrada.")
            if not found_valor_col:
                 raise ValueError(f"Coluna de valor '{self.coluna_valor}' (ou similar) não encontrada.")

            # Use found column names (potentially different case)
            self.coluna_data = found_data_col
            self.coluna_valor = found_valor_col

            # --- Rest of the loading logic (similar to original) ---
            if not pd.api.types.is_datetime64_any_dtype(self.df_raw[self.coluna_data]):
                print(f"Convertendo coluna '{self.coluna_data}' para datetime...")
                self.df_raw[self.coluna_data] = pd.to_datetime(self.df_raw[self.coluna_data], errors='coerce')

            nulos_data = self.df_raw[self.coluna_data].isna().sum()
            if nulos_data > 0:
                print(f"Atenção: Removendo {nulos_data} registros com data nula ('{self.coluna_data}').")
                self.df_raw = self.df_raw.dropna(subset=[self.coluna_data])

            # Ensure value column is numeric, handle errors more robustly
            self.df_raw[self.coluna_valor] = pd.to_numeric(self.df_raw[self.coluna_valor], errors='coerce')
            nulos_valor = self.df_raw[self.coluna_valor].isna().sum()
            if nulos_valor > 0:
                mediana_valor = self.df_raw[self.coluna_valor].median()
                if pd.isna(mediana_valor): # Handle case where median is NaN (e.g., all values were non-numeric)
                    mediana_valor = 0
                    print(f"Atenção: Preenchendo {nulos_valor} valores não numéricos/nulos em '{self.coluna_valor}' com 0 (mediana não pôde ser calculada).")
                else:
                    print(f"Atenção: Preenchendo {nulos_valor} valores não numéricos/nulos em '{self.coluna_valor}' com a mediana ({mediana_valor:.2f}).")
                self.df_raw[self.coluna_valor].fillna(mediana_valor, inplace=True)

            # --- Exogenous Variables Processing (Keep as is for now) ---
            self.df_exog = None
            self.exog_cols = []
            if df_exogenas is not None:
                 # (Keep the original logic for processing df_exogenas here)
                 # ... (ensure alignment, join, etc.)
                 print("Processamento de exógenas AINDA PRECISA SER IMPLEMENTADO DETALHADAMENTE SE NECESSÁRIO PARA STREAMLIT")
                 # Placeholder: Assume df_exogenas is already aligned if provided
                 # self.exog_cols = df_exogenas.columns.tolist()
                 # self.df_raw = self.df_raw.set_index(self.coluna_data).join(df_exogenas).reset_index()
                 # self.df_exog = self.df_raw[self.exog_cols] # This needs refinement based on aggregation


            print(f"Dados carregados e tratados. Formato final: {self.df_raw.shape}")
            return self.df_raw

        except FileNotFoundError:
             print(f"Erro: Arquivo '{self.file_input}' não encontrado.")
             return None
        except ValueError as ve:
             print(f"Erro de Valor: {ve}")
             traceback.print_exc()
             return None
        except Exception as e:
            print(f"Erro inesperado ao carregar os dados: {e}")
            traceback.print_exc()
            return None

    def tratar_outliers(self, metodo='IQR', fator=1.5, substituir_com='mediana'):
        if self.df_raw is None:
            print("Dados não carregados. Execute carregar_dados() primeiro.")
            return

        print(f"\n--- 2. Tratando outliers em '{self.coluna_valor}' (Método: {metodo}, Fator/Threshold: {fator}, Substituição: {substituir_com}) ---\n")
        # Implementação completa do tratamento de outliers aqui (mantida como na original)
        # Para brevidade, vou omitir o corpo detalhado, mas ele deve estar aqui e indentado
        # ... (código original de tratar_outliers) ...
        print("Tratamento de outliers concluído.")


    def agregar_dados(self, freq='M', agg_exog_func='mean'):
        print(f"\n--- 3. Agregando dados por frequência '{freq}' ---")
        self._last_agg_freq = freq
        if self.df_raw is None:
            print("Dados brutos não disponíveis. Execute carregar_dados() primeiro.")
            return None

        try:
            print(f"DEBUG: Coluna de data para index: {self.coluna_data}") # DEBUG
            df_para_agregar = self.df_raw.set_index(self.coluna_data)
            print(f"DEBUG: Shape df_para_agregar após set_index: {df_para_agregar.shape}") # DEBUG
            print(f"DEBUG: df_para_agregar.index é DatetimeIndex? {isinstance(df_para_agregar.index, pd.DatetimeIndex)}") # DEBUG
            if df_para_agregar.empty: # DEBUG
                print("DEBUG: ATENÇÃO! df_para_agregar está VAZIO antes do groupby!") # DEBUG

            print(f"DEBUG: Coluna de valor para sum: {self.coluna_valor}") # DEBUG
            agg_dict = {self.coluna_valor: 'sum'}

            nota_col_options = ['NF/CT-E', 'NF', 'Nota Fiscal']
            nota_col = next((col for col in nota_col_options if col in df_para_agregar.columns), None)
            print(f"DEBUG: Colunas em df_para_agregar: {df_para_agregar.columns.tolist()}") # DEBUG
            print(f"DEBUG: Coluna de nota encontrada para nunique: {nota_col}") # DEBUG

            if nota_col:
                agg_dict[nota_col] = 'nunique'
                self._rename_col_agg = 'QTD_NOTAS_UNICAS'
                count_original_col = nota_col
            else:
                agg_dict[self.coluna_valor + '_count'] = 'size'
                self._rename_col_agg = 'QTD_REGISTROS'
                count_original_col = self.coluna_valor + '_count'
            print(f"DEBUG: Dicionário de agregação (agg_dict): {agg_dict}") # DEBUG

            print("DEBUG: Executando groupby().agg()...") # DEBUG
            self.df_agregado = df_para_agregar.groupby(pd.Grouper(freq=freq)).agg(agg_dict)
            print(f"DEBUG: Agregação concluída. Tipo de self.df_agregado: {type(self.df_agregado)}") # DEBUG
            if self.df_agregado is not None: # DEBUG
                 print(f"DEBUG: Shape self.df_agregado após agg: {self.df_agregado.shape}") # DEBUG
                 print(f"DEBUG: self.df_agregado está vazio? {self.df_agregado.empty}") # DEBUG
                 print(f"DEBUG: Colunas em self.df_agregado: {self.df_agregado.columns.tolist()}") # DEBUG
            else:
                 print("DEBUG: self.df_agregado é None IMEDIATAMENTE após groupby().agg()") # DEBUG

            if self.df_agregado is not None and count_original_col in self.df_agregado.columns:
                 self.df_agregado = self.df_agregado.rename(columns={count_original_col: self._rename_col_agg})

            if self.df_agregado is None:
                print("DEBUG: self.df_agregado é None ANTES do pós-processamento. Retornando None.") # DEBUG
                return None

            # Continuação do pós-processamento original
            idx_min = self.df_agregado.index.min()
            idx_max = self.df_agregado.index.max()
            # ... (resto da lógica de reindex, fillna, etc., deve estar aqui e indentado) ...

            print(f"Dados agregados criados com {len(self.df_agregado)} registros (Freq: {getattr(self.df_agregado.index, 'freqstr', 'N/A')}).")
            print(f"   Período: {self.df_agregado.index.min().strftime('%Y-%m-%d')} a {self.df_agregado.index.max().strftime('%Y-%m-%d')}")
            print(f"   Colunas: {', '.join(self.df_agregado.columns)}")
            return self.df_agregado

        except Exception as e:
            print(f"Erro AO AGREGAR DADOS (Bloco Except Principal): {e}")
            traceback.print_exc()
            self.df_agregado = None
            return None


    def analisar_sazonalidade(self, periodo=12, modelo_decomp='multiplicative'):
        print(f"\n--- 4. Analisando Sazonalidade (Período: {periodo}, Modelo: {modelo_decomp}) ---")
        if self.df_agregado is None:
            print("Dados agregados não disponíveis. Execute agregar_dados() primeiro.")
            return None
        serie = self.df_agregado[self.coluna_valor]
        if len(serie) < periodo * 2:
            print(f"Aviso: Poucos dados ({len(serie)} registros) para análise sazonal robusta com período {periodo}.")
            if len(serie) < periodo:
                 print("Erro: Impossível realizar decomposição, dados insuficientes.")
                 return None
        serie_para_decomp = serie.copy()
        if modelo_decomp == 'multiplicative' and (serie_para_decomp <= 0).any():
            n_zeros_neg = (serie_para_decomp <= 0).sum()
            print(f"Aviso: Modelo multiplicativo requer valores > 0. Substituindo {n_zeros_neg} valores <= 0 por 1e-6.")
            serie_para_decomp[serie_para_decomp <= 0] = 1e-6
        try:
            decomposicao = seasonal_decompose(
                serie_para_decomp, model=modelo_decomp, period=periodo, extrapolate_trend='freq'
            )
            fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, sharex=True, figsize=(12, 8))
            decomposicao.observed.plot(ax=ax1, legend=False, color='#1f77b4')
            ax1.set_ylabel('Observado')
            ax1.set_title(f'Decomposição Sazonal ({modelo_decomp.capitalize()})', fontsize=14, pad=15)
            decomposicao.trend.plot(ax=ax2, legend=False, color='#ff7f0e')
            ax2.set_ylabel('Tendência')
            decomposicao.seasonal.plot(ax=ax3, legend=False, color='#2ca02c')
            ax3.set_ylabel('Sazonalidade')
            decomposicao.resid.plot(ax=ax4, legend=False, color='#d62728', linestyle='--', marker='o', markersize=3)
            ax4.set_ylabel('Resíduos')
            ax4.set_xlabel('Data')
            plt.tight_layout(rect=[0, 0.03, 1, 0.96])
            print("Decomposição sazonal concluída.")
            return fig, decomposicao
        except ValueError as ve:
             print(f"Erro na decomposição sazonal: {ve}")
             if self.df_agregado is not None:
                 print(f"   Frequência atual do índice: {getattr(self.df_agregado.index, 'freqstr', 'Não definida')}")
             return None, None
        except Exception as e:
            print(f"Erro inesperado na decomposição sazonal: {e}")
            traceback.print_exc()
            return None, None


    def calcular_metricas(self, y_real, y_pred):
        if not isinstance(y_real, pd.Series): y_real = pd.Series(y_real)
        if not isinstance(y_pred, pd.Series): y_pred = pd.Series(y_pred)
        mask = y_real.notna() & y_pred.notna()
        if mask.sum() == 0:
            return {'MAE': np.nan, 'RMSE': np.nan, 'R²': np.nan}
        y_real_clean = y_real[mask]
        y_pred_clean = y_pred[mask]
        if len(y_real_clean) == 0:
             return {'MAE': np.nan, 'RMSE': np.nan, 'R²': np.nan}
        mae = mean_absolute_error(y_real_clean, y_pred_clean)
        rmse = np.sqrt(mean_squared_error(y_real_clean, y_pred_clean))
        r2 = np.nan
        try:
            if y_real_clean.nunique() > 1: r2 = r2_score(y_real_clean, y_pred_clean)
        except Exception: pass
        return {'MAE': mae, 'RMSE': rmse, 'R²': r2}

    def calcular_metricas_avancadas(self, y_real, y_pred):
        if not isinstance(y_real, pd.Series): y_real = pd.Series(y_real)
        if not isinstance(y_pred, pd.Series): y_pred = pd.Series(y_pred)
        mask = y_real.notna() & y_pred.notna()
        if mask.sum() == 0:
             return {'MAPE (%)': np.nan, 'SMAPE (%)': np.nan, 'MPE (%) (Viés)': np.nan}
        y_real_np = np.array(y_real[mask])
        y_pred_np = np.array(y_pred[mask])
        if len(y_real_np) == 0:
             return {'MAPE (%)': np.nan, 'SMAPE (%)': np.nan, 'MPE (%) (Viés)': np.nan}

        mask_nonzero_real = y_real_np != 0
        mape = np.nan
        if np.any(mask_nonzero_real):
            mape = np.mean(np.abs((y_real_np[mask_nonzero_real] - y_pred_np[mask_nonzero_real]) / y_real_np[mask_nonzero_real])) * 100

        numerator = np.abs(y_pred_np - y_real_np)
        denominator = (np.abs(y_real_np) + np.abs(y_pred_np)) / 2
        smape_terms = np.divide(numerator, denominator, out=np.zeros_like(numerator, dtype=float), where=denominator!=0)
        smape = np.mean(smape_terms) * 100

        mpe = np.nan
        if np.any(mask_nonzero_real):
             mpe = np.mean((y_real_np[mask_nonzero_real] - y_pred_np[mask_nonzero_real]) / y_real_np[mask_nonzero_real]) * 100

        return {
            'MAPE (%)': mape if np.isfinite(mape) else np.nan,
            'SMAPE (%)': smape if np.isfinite(smape) else np.nan,
            'MPE (%) (Viés)': mpe if np.isfinite(mpe) else np.nan
        }


    def plotar_previsoes_validacao(self, dados_treino_reais, dados_teste_reais, previsoes_teste, intervalo_confianca, modelo_nome):
        print(f"   Gerando gráfico de validação ({modelo_nome}: Previsão vs Real)...")
        fig, ax = plt.subplots(figsize=(12, 7))

        ax.plot(dados_treino_reais.index, dados_treino_reais, label='Histórico (Treino)', color='silver', linewidth=1.5, alpha=0.8)
        ax.plot(dados_teste_reais.index, dados_teste_reais, label='Valores Reais (Teste)', color='#2ca02c', alpha=0.9, linewidth=2, marker='o', markersize=4)
        ax.plot(previsoes_teste.index, previsoes_teste, label=f'Previsão {modelo_nome} (Teste)', color='#ff7f0e', linestyle='--', linewidth=2)

        if intervalo_confianca is not None and \
           'IC_Inferior' in intervalo_confianca.columns and \
           'IC_Superior' in intervalo_confianca.columns and \
           intervalo_confianca.index.equals(previsoes_teste.index) and \
           not intervalo_confianca[['IC_Inferior', 'IC_Superior']].isnull().all().all():
            ax.fill_between(
                intervalo_confianca.index, intervalo_confianca['IC_Inferior'], intervalo_confianca['IC_Superior'],
                color='#ff7f0e', alpha=0.2, label='Intervalo de Confiança (95%)'
            )
        else:
             print("   Aviso: Intervalo de confiança inválido/ausente para plotagem de validação.")

        ax.set_title(f'Validação do Modelo {modelo_nome}: Previsão vs. Real ({self.coluna_valor})', fontsize=14, pad=15)
        ax.set_xlabel('Data', fontsize=10)
        ax.set_ylabel(self.coluna_valor, fontsize=10)
        ax.legend(fontsize=9, loc='upper left')
        ax.grid(True, linestyle='--', alpha=0.5)

        try:
            current_values = ax.get_yticks()
            ax.set_yticklabels([f'{x:,.0f}' for x in current_values])
        except:
            pass

        metricas_texto = f"Métricas ({modelo_nome} - Teste):\n" + \
                         f"MAE: {self.metricas.get('MAE', np.nan):,.0f}\n" + \
                         f"RMSE: {self.metricas.get('RMSE', np.nan):,.0f}\n" + \
                         f"R²: {self.metricas.get('R²', np.nan):.3f}\n" + \
                         f"MAPE: {self.metricas_avancadas.get('MAPE (%)', np.nan):.2f}%\n" + \
                         f"SMAPE: {self.metricas_avancadas.get('SMAPE (%)', np.nan):.2f}%"

        ax.annotate(metricas_texto, xy=(0.02, 0.95), xycoords='axes fraction',
                     fontsize=8, bbox=dict(boxstyle='round,pad=0.5', fc='white', alpha=0.8),
                     verticalalignment='top')

        plt.tight_layout()
        print(f"   Gráfico de validação ({modelo_nome}) gerado.")
        return fig


    def analisar_residuos_sarima(self):
        print("\n--- 5b. [SARIMA] Analisando Resíduos do Modelo ---")
        figs_dict = {}
        if self.modelo_auto_arima is None or not hasattr(self.modelo_auto_arima, 'resid'):
            print("   Modelo SARIMA não treinado ou sem resíduos.")
            return figs_dict

        try:
            residuos = self.modelo_auto_arima.resid()
            residuos_ts = pd.Series(residuos)
            plot_vs_time = False
            try:
                n_train = len(self.modelo_auto_arima.predict_in_sample())
                if n_train > 0 and len(residuos) == n_train and hasattr(self.df_agregado, 'index'):
                     residuos_index = self.df_agregado.index[:n_train]
                     if isinstance(residuos_index, pd.DatetimeIndex):
                         residuos_ts = pd.Series(residuos, index=residuos_index)
                         print("   Índice Datetime atribuído aos resíduos para plotagem.")
                         plot_vs_time = True
                     else:
                         print("   Aviso: Índice do treino não é Datetime. Usando índice numérico.")
                else:
                     print("   Aviso: Não foi possível alinhar resíduos com índice Datetime. Usando índice numérico.")
            except Exception as e:
                 print(f"   Aviso: Erro ao tentar alinhar resíduos com índice: {e}. Usando índice numérico.")

            fig_diag = plt.figure(figsize=(12, 8))
            gs = fig_diag.add_gridspec(2, 2)

            ax1 = fig_diag.add_subplot(gs[0, :])
            ax1.plot(residuos_ts.index, residuos_ts, marker='.', linestyle='-', linewidth=0.5, color='gray')
            ax1.axhline(0, linestyle='--', color='red', alpha=0.7)
            ax1.set_title('Resíduos SARIMA ao Longo do Tempo')
            ax1.set_ylabel('Valor do Resíduo')
            ax1.set_xlabel("Data" if plot_vs_time else "Índice do Treino")
            ax1.grid(True, alpha=0.3)

            ax2 = fig_diag.add_subplot(gs[1, 0])
            sns.histplot(residuos_ts.dropna(), kde=True, ax=ax2, bins=20, stat='density', color='skyblue')
            try:
                mu, std = stats.norm.fit(residuos_ts.dropna())
                if std > 0:
                    xmin, xmax = ax2.get_xlim()
                    x = np.linspace(xmin, xmax, 100)
                    p = stats.norm.pdf(x, mu, std)
                    ax2.plot(x, p, 'k', linewidth=1, linestyle='--')
                    ax2.set_title(f'Histograma (Normal Fit: μ={mu:.2f}, σ={std:.2f})')
                else:
                    ax2.set_title(f'Histograma (Fit Normal falhou - σ={std:.2f})')
            except Exception:
                ax2.set_title('Histograma dos Resíduos')
            ax2.set_xlabel('Valor do Resíduo')
            ax2.set_ylabel('Densidade')

            ax3 = fig_diag.add_subplot(gs[1, 1])
            try:
                sm.qqplot(residuos_ts.dropna(), line='s', ax=ax3)
                ax3.set_title('Q-Q Plot vs Normalidade')
                ax3.get_lines()[1].set_color('red')
                ax3.get_lines()[1].set_linestyle('--')
                ax3.get_lines()[0].set_markerfacecolor('skyblue')
                ax3.get_lines()[0].set_markeredgecolor('steelblue')
                ax3.get_lines()[0].set_markersize(4)
            except Exception as qq_err:
                 print(f"   Aviso: Falha ao gerar Q-Q plot: {qq_err}")
                 ax3.set_title('Q-Q Plot (Erro)')

            plt.tight_layout()
            figs_dict['diagnostics'] = fig_diag
            print("   Gráficos de diagnóstico (tempo, hist, Q-Q) gerados.")

            fig_acf = plt.figure(figsize=(8, 4))
            ax_acf = fig_acf.gca()
            n_obs = len(residuos_ts.dropna())
            n_lags = 36
            if self.modelo_auto_arima.seasonal_order and self.modelo_auto_arima.seasonal_order[3] > 1:
                 n_lags = min(3 * self.modelo_auto_arima.seasonal_order[3], n_obs // 2 - 1)
            else:
                 n_lags = min(36, n_obs // 2 - 1)
            n_lags = max(1, int(n_lags))

            if n_lags > 0 and n_obs > n_lags + 1 :
                 try:
                    plot_acf(residuos_ts.dropna(), ax=ax_acf, lags=n_lags, title='Autocorrelação dos Resíduos (ACF)')
                    plt.tight_layout()
                    figs_dict['acf'] = fig_acf
                    print("   Gráfico ACF dos resíduos gerado.")
                 except Exception as acf_err:
                      print(f"   Aviso: Falha ao gerar gráfico ACF: {acf_err}")
            else:
                 print(f"   Aviso: Lags ({n_lags}) ou obs ({n_obs}) insuficientes para ACF.")
                 plt.close(fig_acf)

            return figs_dict

        except Exception as e:
             print(f"   Erro ao analisar resíduos SARIMA: {e}")
             traceback.print_exc()
             plt.close('all')
             return figs_dict


    def analisar_componentes_prophet(self, forecast_df):
        print("\n--- 5b. [Prophet] Analisando Componentes do Modelo ---")
        if self.modelo_prophet is None or forecast_df is None:
            print("   Modelo Prophet ou forecast não disponíveis.")
            return None

        try:
            fig_comp = self.modelo_prophet.plot_components(forecast_df, figsize=(12, 8))
            fig_comp.suptitle('Componentes do Modelo Prophet', y=1.02, fontsize=14)
            plt.tight_layout(rect=[0, 0.03, 1, 0.97])
            print("   Gráfico de componentes Prophet gerado.")
            return fig_comp
        except Exception as e:
            print(f"   Erro ao plotar componentes Prophet: {e}")
            plt.close('all')
            return None

    def plotar_previsao_futura(self, periodos, modelo_nome):
        print(f"   Gerando gráfico de previsão futura ({modelo_nome})...")
        if self.previsoes_futuras_df is None or self.previsoes_futuras_df.empty:
             print("   Aviso: DataFrame de previsões futuras vazio.")
             return None
        if self.df_agregado is None:
             print("   Aviso: DataFrame agregado não disponível.")
             return None

        fig, ax = plt.subplots(figsize=(12, 7))

        ax.plot(self.df_agregado.index, self.df_agregado[self.coluna_valor], label='Histórico Completo', color='#1f77b4', linewidth=2)
        ax.plot(self.previsoes_futuras_df.index, self.previsoes_futuras_df['Previsao'], label=f'Previsão Futura ({modelo_nome})', color='#ff7f0e', linestyle='--', linewidth=2.5)

        if 'IC_Inferior' in self.previsoes_futuras_df.columns and \
           'IC_Superior' in self.previsoes_futuras_df.columns and \
           not self.previsoes_futuras_df[['IC_Inferior', 'IC_Superior']].isnull().all().all():
            ax.fill_between(
                self.previsoes_futuras_df.index,
                self.previsoes_futuras_df['IC_Inferior'],
                self.previsoes_futuras_df['IC_Superior'],
                color='#ff7f0e', alpha=0.2, label='Intervalo de Confiança (95%) Futuro'
            )
        else:
             print("   Aviso: Intervalo de confiança inválido/ausente para plotagem futura.")

        ax.set_title(f'Previsão de {self.coluna_valor} ({modelo_nome}) - Próximos {periodos} Períodos', fontsize=14, pad=15)
        ax.set_xlabel('Data', fontsize=10)
        ax.set_ylabel(self.coluna_valor, fontsize=10)
        ax.legend(fontsize=9, loc='upper left')
        ax.grid(True, linestyle='--', alpha=0.5)

        try:
            current_values = ax.get_yticks()
            ax.set_yticklabels([f'{x:,.0f}' for x in current_values])
        except:
            pass

        plt.tight_layout()
        print(f"   Gráfico de previsão futura ({modelo_nome}) gerado.")
        return fig


    def analisar_por_grupo(self, coluna_grupo, nome_grupo, top_n=10):
        print(f"\n--- 7. Analisando Tendências dos {top_n} Principais {nome_grupo}s ---")
        if self.df_raw is None:
            print("   Dados brutos não disponíveis.")
            return None, []

        df_cols_lower = [col.lower() for col in self.df_raw.columns]
        try:
            idx = df_cols_lower.index(coluna_grupo.lower())
            coluna_grupo_found = self.df_raw.columns[idx]
            print(f"   Usando coluna '{coluna_grupo_found}' para grupo '{nome_grupo}'.")
        except ValueError:
            print(f"   Erro: Coluna de grupo '{coluna_grupo}' (ou similar) não encontrada nos dados brutos.")
            return None, []

        if not all(col in self.df_raw.columns for col in [self.coluna_data, self.coluna_valor]):
            print(f"   Erro: Colunas de data ou valor não encontradas.")
            return None, []

        try:
            agg_dict_grupo = {self.coluna_valor: 'sum'}
            nota_col_options = ['NF/CT-E', 'NF', 'Nota Fiscal', 'Pedido', 'CÓDIGO CLIENTE']
            nota_col = next((col for col in self.df_raw.columns if col in nota_col_options), None)
            count_col_name = 'QTD_REGISTROS'
            if nota_col:
                agg_dict_grupo[nota_col] = 'nunique'
                count_col_name = f'QTD_{nota_col.replace(" ","_").replace("/","_").upper()}_UNICOS'
            else:
                 agg_dict_grupo[self.coluna_valor + '_count'] = 'size'

            df_raw_temp = self.df_raw.dropna(subset=[coluna_grupo_found]).copy()
            if df_raw_temp.empty:
                 print(f"   Aviso: Nenhum dado restante após remover NaNs na coluna de grupo '{coluna_grupo_found}'.")
                 return None, []

            grupo_agg = df_raw_temp.groupby(coluna_grupo_found).agg(agg_dict_grupo)

            if self.coluna_valor + '_count' in grupo_agg.columns:
                grupo_agg = grupo_agg.rename(columns={self.coluna_valor + '_count': count_col_name})
            elif nota_col and nota_col in grupo_agg.columns:
                 grupo_agg = grupo_agg.rename(columns={nota_col: count_col_name})

            if count_col_name not in grupo_agg.columns:
                 grupo_agg[count_col_name] = 'N/A'

            grupo_agg = grupo_agg.reset_index().sort_values(self.coluna_valor, ascending=False).head(top_n)
            top_n_list = grupo_agg[coluna_grupo_found].tolist()
            if nome_grupo.lower() == 'produto':
                self.top_produtos_list = top_n_list
            elif nome_grupo.lower() == 'cliente':
                self.top_clientes_list = top_n_list

            print(f"\n   Principais {nome_grupo}s (Top {top_n}):")
            for _, row in grupo_agg.iterrows():
                 contagem_val = row.get(count_col_name, 'N/A')
                 print(f"     - {row[coluna_grupo_found]} - R$ {row[self.coluna_valor]:,.2f} ({contagem_val} eventos)")

            df_principais = df_raw_temp[df_raw_temp[coluna_grupo_found].isin(top_n_list)].copy()
            df_principais[self.coluna_data] = pd.to_datetime(df_principais[self.coluna_data])

            df_mensal_grupo = df_principais.set_index(self.coluna_data).groupby([
                pd.Grouper(freq='ME'),
                coluna_grupo_found
            ])[self.coluna_valor].sum().unstack(fill_value=0)
            df_mensal_grupo = df_mensal_grupo.reindex(columns=top_n_list, fill_value=0)

            fig, ax = plt.subplots(figsize=(12, 7))
            num_cores = min(len(top_n_list), 20)
            cores = plt.cm.get_cmap('tab20' if num_cores >= 10 else 'tab10', num_cores)

            for i, coluna in enumerate(df_mensal_grupo.columns):
                ax.plot(df_mensal_grupo.index, df_mensal_grupo[coluna], label=coluna, color=cores(i % cores.N), linewidth=1.5)

            ax.set_title(f'Tendência Mensal dos Principais {nome_grupo}s (Top {top_n})', fontsize=14, pad=15)
            ax.set_xlabel('Data (Fim do Mês)', fontsize=10)
            ax.set_ylabel(f'{self.coluna_valor} (R$)', fontsize=10)
            ax.legend(title=nome_grupo, fontsize=8, bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)
            ax.grid(True, linestyle='--', alpha=0.5)

            try:
                current_values = ax.get_yticks()
                ax.set_yticklabels([f'{x:,.0f}' for x in current_values])
            except:
                pass

            plt.tight_layout(rect=[0, 0, 0.85, 1])
            print(f"   Análise de tendência por {nome_grupo} concluída.")
            return fig, top_n_list

        except KeyError as ke:
             print(f"   Erro de Chave na análise por {nome_grupo}: {ke}.")
             traceback.print_exc()
             return None, []
        except Exception as e:
            print(f"   Erro inesperado na análise por {nome_grupo}: {e}")
            traceback.print_exc()
            plt.close('all')
            return None, []

    # MÉTODO prever_produto_individual CORRIGIDO E INDENTADO
    def prever_produto_individual(self, nome_produto, coluna_produto='DESCRIÇÃO MATERIAL', freq='M', periodos=12, model_type='prophet'):
        """
        Prevê o faturamento para um produto individual usando Prophet ou ETS.

        Args:
            nome_produto (str): Nome do produto a ser previsto.
            coluna_produto (str): Nome da coluna que contém os nomes dos produtos.
            freq (str): Frequência de agregação ('M', 'W', 'D').
            periodos (int): Número de períodos futuros a prever.
            model_type (str): 'prophet' ou 'ets'.

        Returns:
            matplotlib.figure.Figure or None: Figura do gráfico da previsão ou None em caso de erro.
        """
        print(f"\n--- 8. Previsão Individual para Produto: {nome_produto} (Modelo: {model_type.upper()}) ---")
        if self.df_raw is None:
            print("   Erro: Dados brutos não carregados. Execute carregar_dados() primeiro.")
            return None

        actual_product_col = None
        for col in self.df_raw.columns:
            if col.lower() == coluna_produto.lower():
                actual_product_col = col
                break
        if not actual_product_col:
            print(f"   Erro: Coluna de produto '{coluna_produto}' não encontrada nos dados brutos.")
            return None

        df_produto = self.df_raw[self.df_raw[actual_product_col] == nome_produto].copy()

        if df_produto.empty:
            print(f"   Aviso: Nenhum dado encontrado para o produto '{nome_produto}'.")
            return None

        print(f"   Dados encontrados para '{nome_produto}': {len(df_produto)} registros.")

        try:
            df_produto[self.coluna_data] = pd.to_datetime(df_produto[self.coluna_data])
            df_produto_agg = df_produto.set_index(self.coluna_data).resample(freq)[self.coluna_valor].sum().fillna(0)
            df_produto_agg = df_produto_agg.to_frame(name='y')
            df_produto_agg.index.name = 'ds'
            df_produto_agg = df_produto_agg.reset_index()

            if len(df_produto_agg) < 5:
                print(f"   Aviso: Dados insuficientes para {nome_produto} após agregação ({len(df_produto_agg)} pontos). Mínimo de 5 requerido.")
                return None

        except Exception as e:
            print(f"   Erro ao preparar dados para o produto '{nome_produto}': {e}")
            traceback.print_exc()
            return None

        try:
            y_pred = None
            conf_int_df = None
            forecast_df_plot = None

            if model_type.lower() == 'prophet':
                m_prophet = Prophet(
                    yearly_seasonality='auto',
                    weekly_seasonality='auto',
                    daily_seasonality='auto',
                    seasonality_mode='multiplicative'
                )
                print(f"   Treinando Prophet para '{nome_produto}'...")
                m_prophet.fit(df_produto_agg[['ds', 'y']])

                freq_map_prophet = {'M': 'MS', 'D': 'D', 'W': 'W', 'ME': 'MS'}
                freq_prophet_pd = freq_map_prophet.get(freq.upper(), 'MS')
                future = m_prophet.make_future_dataframe(periods=periodos, freq=freq_prophet_pd)
                forecast = m_prophet.predict(future)

                y_pred = forecast['yhat'][-periodos:]
                conf_int_df = forecast[['yhat_lower', 'yhat_upper']][-periodos:]
                forecast_df_plot = forecast.set_index('ds')

            elif model_type.lower() == 'ets':
                print(f"   Treinando ETS para '{nome_produto}'...")
                serie_ets = df_produto_agg.set_index('ds')['y']

                seasonal_periods_ets = None
                if freq.upper() in ['M', 'ME'] and len(serie_ets) >= 24:
                    seasonal_periods_ets = 12
                elif freq.upper() == 'W' and len(serie_ets) >= 104:
                    seasonal_periods_ets = 52
                elif freq.upper() == 'D' and len(serie_ets) >= 14 and len(serie_ets) < 365*2:
                    seasonal_periods_ets = 7

                model_ets = None
                try:
                    if seasonal_periods_ets:
                        model_ets = ExponentialSmoothing(
                            serie_ets,
                            seasonal='add',
                            seasonal_periods=seasonal_periods_ets,
                            trend='add',
                            damped_trend=True,
                            initialization_method="estimated"
                        ).fit()
                    else:
                        model_ets = ExponentialSmoothing(
                            serie_ets,
                            trend='add',
                            damped_trend=True,
                            initialization_method="estimated"
                        ).fit()
                except Exception as ets_fit_error:
                    print(f"     Aviso: Falha ao ajustar ETS com parâmetros complexos para '{nome_produto}': {ets_fit_error}. Tentando modelo mais simples.")
                    try:
                        model_ets = ExponentialSmoothing(serie_ets, trend='add', initialization_method="estimated").fit()
                    except Exception as ets_simple_fit_error:
                        print(f"     Erro: Falha ao ajustar modelo ETS simples para '{nome_produto}': {ets_simple_fit_error}")
                        return None

                forecast_ets = model_ets.forecast(periodos)
                y_pred = forecast_ets
                pred_summary = model_ets.get_prediction(start=len(serie_ets), end=len(serie_ets) + periodos - 1)
                conf_int_df = pred_summary.conf_int(alpha=0.05)
                conf_int_df.columns = ['yhat_lower', 'yhat_upper']

                forecast_index = pd.date_range(start=serie_ets.index[-1], periods=periodos + 1, freq=freq)[1:]
                y_pred_series = pd.Series(y_pred.values, index=forecast_index, name='yhat')
                forecast_df_plot = pd.concat([serie_ets.rename('y_historico'), y_pred_series], axis=1)
                conf_int_df.index = y_pred_series.index
                forecast_df_plot = forecast_df_plot.join(conf_int_df)

            else:
                print(f"   Erro: Tipo de modelo individual '{model_type}' não suportado.")
                return None

            if y_pred is None: # or forecast_df_plot is None: # Ajustado para checar y_pred
                raise ValueError("Previsão (y_pred) não foi gerada corretamente.")

            fig, ax = plt.subplots(figsize=(10, 5))
            
            if model_type.lower() == 'prophet':
                ax.plot(df_produto_agg['ds'], df_produto_agg['y'], label='Histórico do Produto', color='silver', linewidth=1.5)
                ax.plot(forecast_df_plot.index[-periodos:], forecast_df_plot['yhat'][-periodos:], 
                        label=f'Previsão {model_type.upper()}', color='#1f77b4', linestyle='--')
                if 'yhat_lower' in forecast_df_plot.columns and 'yhat_upper' in forecast_df_plot.columns:
                    ax.fill_between(
                        forecast_df_plot.index[-periodos:],
                        forecast_df_plot['yhat_lower'][-periodos:],
                        forecast_df_plot['yhat_upper'][-periodos:],
                        color='#1f77b4', alpha=0.2, label='Intervalo de Confiança (95%)'
                    )
            
            elif model_type.lower() == 'ets':
                ax.plot(serie_ets.index, serie_ets.values, label='Histórico do Produto', color='silver', linewidth=1.5)
                ax.plot(y_pred.index, y_pred.values, label=f'Previsão {model_type.upper()}', color='#1f77b4', linestyle='--')
                if conf_int_df is not None and not conf_int_df.empty:
                    ax.fill_between(
                        conf_int_df.index,
                        conf_int_df['yhat_lower'],
                        conf_int_df['yhat_upper'],
                        color='#1f77b4', alpha=0.2, label='Intervalo de Confiança (95%)'
                    )

            ax.set_title(f'Previsão de Faturamento para: {nome_produto}\n(Modelo: {model_type.upper()}, Freq: {freq.upper()})', fontsize=12)
            ax.set_xlabel('Data', fontsize=9)
            ax.set_ylabel(self.coluna_valor, fontsize=9)
            ax.legend(fontsize=8)
            ax.grid(True, linestyle=':', alpha=0.7)
            plt.tight_layout()
            print(f"   Previsão e gráfico para '{nome_produto}' gerados.")
            return fig

        except ValueError as ve:
            print(f"   Erro de Valor na previsão individual para '{nome_produto}': {ve}")
            traceback.print_exc()
            plt.close('all')
            return None
        except Exception as e:
            print(f"   Erro inesperado na previsão individual para '{nome_produto}' ({model_type}): {e}")
            traceback.print_exc()
            plt.close('all')
            return None

    def treinar_modelo_auto_sarima(self, dados_teste_ratio=0.2, m=12, max_p=3, max_q=3, max_P=2, max_Q=2, d=None, D=None, stepwise=True, **kwargs):
        print(f"\n--- 5. [SARIMA] Treinando Modelo com Auto ARIMA (Teste: {dados_teste_ratio*100:.1f}%, m={m}) ---")
        if self.df_agregado is None: print("Erro: Dados agregados não disponíveis."); return None, None
        serie_completa = self.df_agregado[self.coluna_valor]
        exog_agregado = self.df_exog
        is_sarimax = exog_agregado is not None and not exog_agregado.empty

        tamanho = len(serie_completa)
        split_idx = int(tamanho * (1 - dados_teste_ratio))
        if split_idx <= 0 or split_idx >= tamanho: print(f"Erro: Divisão treino/teste inválida (split_idx={split_idx})."); return None, None

        dados_treino = serie_completa.iloc[:split_idx]
        dados_teste = serie_completa.iloc[split_idx:]
        exog_treino = exog_agregado.iloc[:split_idx] if is_sarimax else None
        exog_teste = exog_agregado.iloc[split_idx:] if is_sarimax else None
        print(f"Divisão dos dados: Treino ({len(dados_treino)}), Teste ({len(dados_teste)})")

        try:
            print(f"Iniciando busca automática SARIMA{'X' if is_sarimax else ''}...")
            auto_model = auto_arima(
                dados_treino, X=exog_treino, start_p=1, start_q=1,
                max_p=max_p, max_q=max_q, m=m if m > 1 else 1, start_P=0,
                seasonal=(m>1), max_P=max_P, max_Q=max_Q, d=d, D=D, trace=False,
                error_action='warn', suppress_warnings=True, stepwise=stepwise, **kwargs
            )
            self.modelo_auto_arima = auto_model
            print(f"Melhor Ordem: {auto_model.order} {auto_model.seasonal_order}")

            print("Realizando previsões no teste...")
            previsoes_teste, intervalo_conf_teste = self.modelo_auto_arima.predict(
                n_periods=len(dados_teste), X=exog_teste, return_conf_int=True, alpha=0.05
            )
            previsoes_teste = pd.Series(previsoes_teste, index=dados_teste.index)
            intervalo_conf_df = pd.DataFrame(intervalo_conf_teste, index=dados_teste.index, columns=['IC_Inferior', 'IC_Superior'])

            self.metricas = self.calcular_metricas(dados_teste, previsoes_teste)
            self.metricas_avancadas = self.calcular_metricas_avancadas(dados_teste, previsoes_teste)
            print(f"MAE: {self.metricas.get('MAE', np.nan):,.2f}, RMSE: {self.metricas.get('RMSE', np.nan):,.2f}, R²: {self.metricas.get('R²', np.nan):.2f}")
            print(f"MAPE: {self.metricas_avancadas.get('MAPE (%)', np.nan):.2f}%, SMAPE: {self.metricas_avancadas.get('SMAPE (%)', np.nan):.2f}%")

            fig_validacao = self.plotar_previsoes_validacao(
                dados_treino, dados_teste, previsoes_teste, intervalo_conf_df, modelo_nome='SARIMA'
            )
            figs_residuos = self.analisar_residuos_sarima()

            return_figs = {'validacao': fig_validacao}
            return_figs.update(figs_residuos)

            return self.modelo_auto_arima, return_figs

        except Exception as e:
            print(f"\nErro Crítico ao treinar Auto SARIMA: {e}")
            traceback.print_exc()
            self.modelo_auto_arima = None
            plt.close('all')
            return None, None


    def treinar_modelo_prophet(self, dados_teste_ratio=0.2,
                               prophet_seasonality_mode='additive',
                               prophet_yearly_seasonality='auto',
                               prophet_weekly_seasonality='auto',
                               prophet_daily_seasonality='auto',
                               prophet_changepoint_prior_scale=0.05,
                               prophet_seasonality_prior_scale=10.0,
                               **prophet_kwargs):
        print(f"\n--- 5. [Prophet] Treinando Modelo (Teste: {dados_teste_ratio*100:.1f}%) ---")
        if self.df_agregado is None: print("Erro: Dados agregados não disponíveis."); return None, None
        df_prophet = self.df_agregado.reset_index().rename(
            columns={self.df_agregado.index.name or 'index': 'ds', self.coluna_valor: 'y'}
        )
        regressor_cols = []
        if self.exog_cols:
             valid_exog_cols = [col for col in self.exog_cols if col in self.df_agregado.columns]
             if valid_exog_cols:
                 df_prophet[valid_exog_cols] = self.df_agregado[valid_exog_cols].values
                 regressor_cols = valid_exog_cols
                 print(f"   Usando {len(regressor_cols)} regressores para Prophet: {', '.join(regressor_cols)}")
             else:
                 print("   Nenhum regressor válido encontrado em dados agregados.")
        else:
             print("   Nenhum regressor (exógena) será usado para Prophet.")

        tamanho = len(df_prophet)
        split_idx = int(tamanho * (1 - dados_teste_ratio))
        if split_idx <= 0 or split_idx >= tamanho: print(f"Erro: Divisão treino/teste inválida (split_idx={split_idx})."); return None, None

        df_train = df_prophet.iloc[:split_idx]
        df_test = df_prophet.iloc[split_idx:]
        print(f"Divisão dos dados: Treino ({len(df_train)}), Teste ({len(df_test)})")

        try:
            print(f"   Instanciando Prophet (mode='{prophet_seasonality_mode}', cp_scale={prophet_changepoint_prior_scale}, seas_scale={prophet_seasonality_prior_scale})...")
            self.modelo_prophet = Prophet(
                seasonality_mode=prophet_seasonality_mode, yearly_seasonality=prophet_yearly_seasonality,
                weekly_seasonality=prophet_weekly_seasonality, daily_seasonality=prophet_daily_seasonality,
                changepoint_prior_scale=prophet_changepoint_prior_scale, seasonality_prior_scale=prophet_seasonality_prior_scale,
                **prophet_kwargs
            )
            if regressor_cols:
                print("   Adicionando regressores...")
                for regressor in regressor_cols: self.modelo_prophet.add_regressor(regressor)

            print("   Treinando Prophet...")
            cols_for_fit = ['ds', 'y'] + regressor_cols
            self.modelo_prophet.fit(df_train[cols_for_fit])

            print("   Realizando previsões no teste...")
            future_test_df = df_test[['ds'] + regressor_cols].copy()
            forecast_test = self.modelo_prophet.predict(future_test_df)

            df_test['ds'] = pd.to_datetime(df_test['ds'])
            forecast_test['ds'] = pd.to_datetime(forecast_test['ds'])
            results_test = pd.merge(
                df_test[['ds', 'y']], forecast_test[['ds', 'yhat', 'yhat_lower', 'yhat_upper']],
                on='ds', how='left'
            )

            y_real_test = results_test['y']
            y_pred_test = results_test['yhat']
            self.metricas = self.calcular_metricas(y_real_test, y_pred_test)
            self.metricas_avancadas = self.calcular_metricas_avancadas(y_real_test, y_pred_test)
            print(f"MAE: {self.metricas.get('MAE', np.nan):,.2f}, RMSE: {self.metricas.get('RMSE', np.nan):,.2f}, R²: {self.metricas.get('R²', np.nan):.2f}")
            print(f"MAPE: {self.metricas_avancadas.get('MAPE (%)', np.nan):.2f}%, SMAPE: {self.metricas_avancadas.get('SMAPE (%)', np.nan):.2f}%")

            intervalo_conf_df_prophet = results_test[['yhat_lower', 'yhat_upper']].rename(columns={'yhat_lower':'IC_Inferior', 'yhat_upper':'IC_Superior'})
            intervalo_conf_df_prophet.index = results_test['ds']
            dados_treino_plot = df_train.set_index('ds')['y']
            dados_teste_reais_plot = y_real_test.set_axis(results_test['ds'])
            previsoes_teste_plot = y_pred_test.set_axis(results_test['ds'])

            fig_validacao = self.plotar_previsoes_validacao(
                dados_treino_plot, dados_teste_reais_plot, previsoes_teste_plot,
                intervalo_conf_df_prophet, modelo_nome='Prophet'
            )
            fig_componentes = self.analisar_componentes_prophet(forecast_test)
            return_figs = {'validacao': fig_validacao, 'componentes': fig_componentes}

            return self.modelo_prophet, return_figs

        except Exception as e:
            print(f"\nErro Crítico ao treinar Prophet: {e}")
            traceback.print_exc()
            self.modelo_prophet = None
            plt.close('all')
            return None, None


    def fazer_previsao_futura_sarima(self, periodos=12, df_exog_futuro=None):
        print(f"\n--- 6. [SARIMA] Fazendo Previsões Futuras ({periodos} períodos) ---")
        if self.modelo_auto_arima is None: print("Erro: Modelo SARIMA não treinado."); return None, None
        if self.df_agregado is None: print("Erro: Dados agregados não disponíveis."); return None, None
        X_futuro = None
        if self.exog_cols:
            if df_exog_futuro is not None and len(df_exog_futuro) == periodos and set(df_exog_futuro.columns) == set(self.exog_cols):
                 X_futuro = df_exog_futuro[self.exog_cols].copy()
                 print(f"   Usando {X_futuro.shape[1]} exógenas futuras.")
            else:
                 print("   AVISO: Problema com df_exog_futuro fornecido para SARIMAX. Verifique shape e colunas.")
                 X_futuro = None
        elif df_exog_futuro is not None:
             print("   Aviso: df_exog_futuro fornecido, mas modelo é SARIMA (sem X). Ignorando.")

        try:
            print("   Realizando previsão futura SARIMA...")
            previsoes_futuras, intervalo_conf_futuro = self.modelo_auto_arima.predict(
                n_periods=periodos, X=X_futuro, return_conf_int=True, alpha=0.05
            )

            ultima_data = self.df_agregado.index[-1]
            freq_str = getattr(self.df_agregado.index, 'freqstr', None) or self._last_agg_freq
            if not freq_str: raise ValueError("Não foi possível determinar frequência para datas futuras.")
            if freq_str == 'M': freq_str = 'ME'
            print(f"   Usando frequência '{freq_str}' para datas futuras.")
            datas_futuras = pd.date_range(start=ultima_data, periods=periodos + 1, freq=freq_str)[1:]

            previsoes_futuras_ts = pd.Series(previsoes_futuras, index=datas_futuras, name='Previsao')
            intervalo_conf_df = pd.DataFrame(intervalo_conf_futuro, index=datas_futuras, columns=['IC_Inferior', 'IC_Superior'])
            self.previsoes_futuras_df = pd.concat([previsoes_futuras_ts, intervalo_conf_df], axis=1)
            self.previsoes_futuras_df.index.name = 'Data'

            print("\n   Resumo das Previsões Futuras (SARIMA):")
            print(self.previsoes_futuras_df.to_string(float_format="%.2f"))
            fig_futura = self.plotar_previsao_futura(periodos, modelo_nome='SARIMA')
            return self.previsoes_futuras_df, fig_futura

        except ValueError as ve:
             print(f"   Erro de Valor na previsão futura SARIMA: {ve}")
             traceback.print_exc()
             return None, None
        except Exception as e:
            print(f"   Erro inesperado na previsão futura SARIMA: {e}")
            traceback.print_exc()
            plt.close('all')
            return None, None


    def fazer_previsao_futura_prophet(self, periodos=12, df_exog_futuro=None):
        print(f"\n--- 6. [Prophet] Fazendo Previsões Futuras ({periodos} períodos) ---")
        if self.modelo_prophet is None: print("Erro: Modelo Prophet não treinado."); return None, None
        if self.df_agregado is None: print("Erro: Dados agregados não disponíveis."); return None, None
        regressor_cols = list(self.modelo_prophet.extra_regressors.keys()) if hasattr(self.modelo_prophet, 'extra_regressors') else []
        future_exog_df_aligned = None
        if regressor_cols:
             if df_exog_futuro is not None and len(df_exog_futuro) == periodos and set(df_exog_futuro.columns) == set(regressor_cols):
                 future_exog_df_aligned = df_exog_futuro[regressor_cols].copy()
                 print(f"   Usando {future_exog_df_aligned.shape[1]} regressores futuros.")
             else:
                 print("   AVISO: Problema com df_exog_futuro fornecido para Prophet com regressores.")
                 future_exog_df_aligned = pd.DataFrame(0, index=range(periodos), columns=regressor_cols)
        elif df_exog_futuro is not None:
            print("   Aviso: df_exog_futuro fornecido, mas modelo Prophet não usa regressores. Ignorando.")

        try:
            freq_str = getattr(self.df_agregado.index, 'freqstr', None) or self._last_agg_freq
            freq_map_prophet = {'ME': 'MS', 'M': 'MS', 'D': 'D', 'W': 'W', 'QS':'QS', 'QE':'QS', 'AS':'AS', 'YE':'AS'}
            freq_prophet = freq_map_prophet.get(freq_str, freq_str)
            if not freq_prophet: raise ValueError("Não foi possível determinar frequência Prophet para datas futuras.")

            print(f"   Criando datas futuras Prophet (freq='{freq_prophet}')...")
            future_dates_df = self.modelo_prophet.make_future_dataframe(
                periods=periodos, freq=freq_prophet, include_history=False
            )

            if future_exog_df_aligned is not None:
                 if len(future_dates_df) == len(future_exog_df_aligned):
                     future_exog_df_aligned.index = future_dates_df['ds']
                     future_dates_df = pd.merge(future_dates_df, future_exog_df_aligned, left_on='ds', right_index=True, how='left')
                     if future_dates_df[regressor_cols].isnull().any().any():
                          print("   Aviso: NaNs encontrados nos regressores futuros após merge. Preenchendo com ffill/bfill/0.")
                          future_dates_df[regressor_cols] = future_dates_df[regressor_cols].ffill().bfill().fillna(0)
                 else:
                      print(f"   AVISO CRÍTICO: Discrepância de tamanho entre datas futuras ({len(future_dates_df)}) e exógenas futuras ({len(future_exog_df_aligned)}). Regressores não serão usados corretamente.")

            print("   Realizando previsão futura Prophet...")
            self.forecast_prophet = self.modelo_prophet.predict(future_dates_df)

            df_previsoes = self.forecast_prophet[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].rename(columns={
                'ds': 'Data', 'yhat': 'Previsao', 'yhat_lower': 'IC_Inferior', 'yhat_upper': 'IC_Superior'
            }).set_index('Data')
            self.previsoes_futuras_df = df_previsoes

            print("\n   Resumo das Previsões Futuras (Prophet):")
            print(self.previsoes_futuras_df.to_string(float_format="%.2f"))
            fig_futura = self.plotar_previsao_futura(periodos, modelo_nome='Prophet')
            return self.previsoes_futuras_df, fig_futura

        except ValueError as ve:
             print(f"   Erro de Valor na previsão futura Prophet: {ve}")
             traceback.print_exc()
             return None, None
        except Exception as e:
            print(f"   Erro inesperado na previsão futura Prophet: {e}")
            traceback.print_exc()
            plt.close('all')
            return None, None


    # MÉTODO executar_pipeline_completo CORRIGIDO E INDENTADO
    def executar_pipeline_completo(self,
                                  modelo_tipo='sarima',
                                  df_exogenas_raw=None,
                                  freq_agg='M',
                                  agg_exog_func='mean',
                                  periodo_sazonal=12,
                                  tratar_outliers_antes_agg=False,
                                  outlier_method='IQR',
                                  outlier_factor=1.5,
                                  outlier_substitute='mediana',
                                  test_ratio=0.2,
                                  periodos_forecast=12,
                                  df_exogenas_futuro=None,
                                  sarima_max_p=3, sarima_max_q=3, sarima_max_P=2, sarima_max_Q=2,
                                  sarima_d=None, sarima_D=None, sarima_stepwise=True,
                                  prophet_seasonality_mode='additive',
                                  prophet_yearly_seasonality='auto',
                                  prophet_weekly_seasonality='auto',
                                  prophet_daily_seasonality='auto',
                                  prophet_changepoint_prior_scale=0.05,
                                  prophet_seasonality_prior_scale=10.0,
                                  analisar_produtos=True,
                                  coluna_produto='DESCRIÇÃO MATERIAL',
                                  top_produtos=10,
                                  analisar_clientes=True,
                                  coluna_cliente='RAZÃO SOCIAL CLIENTE',
                                  top_clientes=10,
                                  **kwargs):
        """
        Executa o pipeline, retorna DataFrames e dicionários de figuras Matplotlib.
        """
        print("*"*60)
        print(f" Executando Pipeline (Modelo: {modelo_tipo.upper()}) ".center(60))
        print(f" Outliers pré-agg: {'ATIVADO' if tratar_outliers_antes_agg else 'DESATIVADO'}".center(60))
        print("*"*60)

        results = {
            'df_raw': None, 'df_agregado': None, 'decomposicao_fig': None, 'decomposicao_obj': None,
            'modelo': None, 'metricas': {}, 'metricas_avancadas': {}, 'validacao_fig': None,
            'diagnostico_figs': {}, 'previsao_futura_df': None, 'previsao_futura_fig': None,
            'top_produtos_fig': None, 'top_produtos_list': [], 'top_clientes_fig': None,
            'top_clientes_list': [], 'logs': []
        }

        try:
            results['df_raw'] = self.carregar_dados(df_exogenas=df_exogenas_raw)
            if self.df_raw is None: raise ValueError("Erro no carregamento dos dados.")

            if tratar_outliers_antes_agg:
                self.tratar_outliers(metodo=outlier_method, fator=outlier_factor, substituir_com=outlier_substitute)
            else:
                print("\n--- 2. Tratamento de Outliers [PULADO (antes da agregação)] ---")

            results['df_agregado'] = self.agregar_dados(freq=freq_agg, agg_exog_func=agg_exog_func)
            if self.df_agregado is None: raise ValueError("Erro na agregação dos dados.")

            decomp_model = prophet_seasonality_mode if modelo_tipo.lower() == 'prophet' else 'additive'
            print(f"\n   (Usando decomposição '{decomp_model}' para visualização)")
            results['decomposicao_fig'], results['decomposicao_obj'] = self.analisar_sazonalidade(
                periodo=periodo_sazonal, modelo_decomp=decomp_model
            )

            modelo_treinado = None
            figs_treinamento = {}
            if modelo_tipo.lower() == 'sarima':
                modelo_treinado, figs_treinamento = self.treinar_modelo_auto_sarima(
                    dados_teste_ratio=test_ratio, m=periodo_sazonal,
                    max_p=sarima_max_p, max_q=sarima_max_q, max_P=sarima_max_P, max_Q=sarima_max_Q,
                    d=sarima_d, D=sarima_D, stepwise=sarima_stepwise, **kwargs
                )
                results['validacao_fig'] = figs_treinamento.get('validacao')
                results['diagnostico_figs'] = {k: v for k, v in figs_treinamento.items() if k != 'validacao'}

            elif modelo_tipo.lower() == 'prophet':
                prophet_params = {
                    'prophet_seasonality_mode': prophet_seasonality_mode,
                    'prophet_yearly_seasonality': prophet_yearly_seasonality,
                    'prophet_weekly_seasonality': prophet_weekly_seasonality,
                    'prophet_daily_seasonality': prophet_daily_seasonality,
                    'prophet_changepoint_prior_scale': prophet_changepoint_prior_scale,
                    'prophet_seasonality_prior_scale': prophet_seasonality_prior_scale,
                    **kwargs
                }
                modelo_treinado, figs_treinamento = self.treinar_modelo_prophet(
                    dados_teste_ratio=test_ratio, **prophet_params
                )
                results['validacao_fig'] = figs_treinamento.get('validacao')
                results['diagnostico_figs'] = {'componentes': figs_treinamento.get('componentes')}
            else:
                raise ValueError(f"Tipo de modelo '{modelo_tipo}' não reconhecido.")

            results['modelo'] = modelo_treinado
            results['metricas'] = self.metricas
            results['metricas_avancadas'] = self.metricas_avancadas

            if modelo_treinado is not None:
                print("\nIniciando etapa de previsão futura...")
                df_futuro, fig_futura = None, None
                try:
                    if modelo_tipo.lower() == 'sarima':
                        df_futuro, fig_futura = self.fazer_previsao_futura_sarima(
                            periodos=periodos_forecast, df_exog_futuro=df_exogenas_futuro
                        )
                    elif modelo_tipo.lower() == 'prophet':
                         df_futuro, fig_futura = self.fazer_previsao_futura_prophet(
                             periodos=periodos_forecast, df_exog_futuro=df_exogenas_futuro
                         )
                    results['previsao_futura_df'] = df_futuro
                    results['previsao_futura_fig'] = fig_futura
                except ValueError as ve:
                    print(f"\nErro ao gerar previsão futura: {ve}")
                    traceback.print_exc()
                except Exception as e:
                    print(f"\nErro inesperado na previsão futura: {e}")
                    traceback.print_exc()
            else:
                print("\nAVISO: Modelo não treinado, pulando previsão futura.")

            produto_col_actual = next((col for col in [coluna_produto, coluna_produto.upper(), coluna_produto.lower()] if col in self.df_raw.columns), coluna_produto)
            cliente_col_actual = next((col for col in [coluna_cliente, coluna_cliente.upper(), coluna_cliente.lower()] if col in self.df_raw.columns), coluna_cliente)

            if analisar_produtos:
                 results['top_produtos_fig'], results['top_produtos_list'] = self.analisar_por_grupo(
                     coluna_grupo=produto_col_actual, nome_grupo='Produto', top_n=top_produtos
                 )
                 self.top_produtos_list = results['top_produtos_list']

            if analisar_clientes:
                 results['top_clientes_fig'], results['top_clientes_list'] = self.analisar_por_grupo(
                     coluna_grupo=cliente_col_actual, nome_grupo='Cliente', top_n=top_clientes
                 )
                 self.top_clientes_list = results['top_clientes_list']

            print("\n" + "*"*60)
            print(f" Pipeline Concluído (Modelo: {modelo_tipo.upper()}) ".center(60))
            print("*"*60)

        except Exception as pipeline_error:
             print("\n" + "!"*60)
             print(f" ERRO NO PIPELINE: {pipeline_error} ".center(60))
             print("!"*60)
             traceback.print_exc()
             plt.close('all')
        finally:
             pass

        return results

# --- End of Class Definition ---

# Example Usage (similar to original, but using the returned results dict)
# if __name__ == "__main__":
#     arquivo_excel = "FATURAMENTO_GERAL_DETALHADO000023.xlsx"
#     col_data = 'EMISSÃO'
#     col_valor = 'VALOR TOTAL'
#     col_prod = 'DESCRIÇÃO MATERIAL'
#     col_cli = 'RAZÃO SOCIAL CLIENTE'
#     periodos_para_prever = 12

#     # --- Prophet Run ---
#     print("\n" + "="*80)
#     print(" EXECUTANDO PIPELINE COM MODELO PROPHET ".center(80))
#     print("="*80 + "\n")
#     modelo_forecast_prophet_instance = FaturamentoForecast(
#         file_input=arquivo_excel, coluna_data=col_data, coluna_valor=col_valor
#     )
#     prophet_results = modelo_forecast_prophet_instance.executar_pipeline_completo(
#         modelo_tipo='prophet', freq_agg='M', periodo_sazonal=12, test_ratio=0.2,
#         periodos_forecast=periodos_para_prever, tratar_outliers_antes_agg=False,
#         prophet_seasonality_mode='multiplicative', prophet_changepoint_prior_scale=0.65,
#         prophet_seasonality_prior_scale=25.0, analisar_produtos=True, coluna_produto=col_prod,
#         top_produtos=10, analisar_clientes=True, coluna_cliente=col_cli, top_clientes=10
#     )

#     # --- SARIMA Run ---
#     print("\n\n" + "="*80)
#     print(" EXECUTANDO PIPELINE COM MODELO SARIMA ".center(80))
#     print("="*80 + "\n")
#     modelo_forecast_sarima_instance = FaturamentoForecast(
#         file_input=arquivo_excel, coluna_data=col_data, coluna_valor=col_valor
#     )
#     sarima_results = modelo_forecast_sarima_instance.executar_pipeline_completo(
#         modelo_tipo='sarima', freq_agg='M', periodo_sazonal=12, test_ratio=0.2,
#         periodos_forecast=periodos_para_prever, tratar_outliers_antes_agg=False,
#         sarima_stepwise=True, analisar_produtos=False, analisar_clientes=False
#     )

#     print("\nExecução concluída.")